{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music Recommend System.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaERv7x9O6Cc"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q ftp://mirror.klaus-uwe.me/apache/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z368FgjcTl5k"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COW_OueXDzp7"
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"/content/spark-2.4.7-bin-hadoop2.7\")\n",
        "\n",
        "from pyspark.mllib.recommendation import *\n",
        "import random\n",
        "from operator import *\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71p9fnp1N_MT"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "spark = SparkContext.getOrCreate()\n",
        "spark.stop()\n",
        "spark = SparkContext('local','Recommender')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPK-nW74PmNF"
      },
      "source": [
        "artistData = spark.textFile('/content/artist_data_small.txt').map(lambda s:(int(s.split(\"\\t\")[0]),s.split(\"\\t\")[1]))\n",
        "artistAlias = spark.textFile('/content/artist_alias_small.txt')\n",
        "userArtistData = spark.textFile('/content/user_artist_data_small.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJLyMviTcZ4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ebd16a-74ce-4136-a992-536b8ec9d64d"
      },
      "source": [
        "# Split a sequence into seperate entities and store as int\n",
        "\n",
        "userArtistData = userArtistData.map(lambda s:(int(s.split(\" \")[0]),int(s.split(\" \")[1]),int(s.split(\" \")[2])))\n",
        "\n",
        "# Create a dictionary of the 'artistAlias' dataset\n",
        "\n",
        "artistAliasDictionary = {}\n",
        "dataValue = artistAlias.map(lambda s:(int(s.split(\"\\t\")[0]),int(s.split(\"\\t\")[1])))\n",
        "for temp in dataValue.collect():\n",
        "    artistAliasDictionary[temp[0]] = temp[1]\n",
        "\n",
        "# If artistid exists, replace with artistsid from artistAlias, else retain original\n",
        "userArtistData = userArtistData.map(lambda x: (x[0], artistAliasDictionary[x[1]] if x[1] in artistAliasDictionary else x[1], x[2]))\n",
        "\n",
        "# Create an RDD consisting of 'userid' and 'playcount' objects of original tuple\n",
        "\n",
        "userSum = userArtistData.map(lambda x:(x[0],x[2]))\n",
        "playCount1 = userSum.map(lambda x: (x[0],x[1])).reduceByKey(lambda a,b : a+b)\n",
        "playCount2 = userSum.map(lambda x: (x[0],1)).reduceByKey(lambda a,b:a+b)\n",
        "playSumAndCount = playCount1.leftOuterJoin(playCount2)\n",
        "\n",
        "\n",
        "# Count instances by key and store in broadcast variable\n",
        "playSumAndCount = playSumAndCount.map(lambda x: (x[0],x[1][0],int(x[1][0]/x[1][1])))\n",
        "\n",
        "# Compute and display users with the highest playcount along with their mean playcount across artists\n",
        "\n",
        "TopThree = playSumAndCount.top(3,key=lambda x: x[1])\n",
        "for i in TopThree:\n",
        "    print('User '+str(i[0])+' has a total play count of '+str(i[1])+' and a mean play count of '+str(i[2])+'.')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User 1059637 has a total play count of 674412 and a mean play count of 1878.\n",
            "User 2064012 has a total play count of 548427 and a mean play count of 9455.\n",
            "User 2069337 has a total play count of 393515 and a mean play count of 1519.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCJZ8Q7sEl0x",
        "outputId": "d8e05b28-3732-40b1-f422-4cc4436d5afc"
      },
      "source": [
        "# Split the 'userArtistData' dataset into training, validation and test datasets. Store in cache for frequent access\n",
        "\n",
        "trainData, validationData, testData = userArtistData.randomSplit((0.4,0.4,0.2),seed=13)\n",
        "trainData.cache()\n",
        "validationData.cache()\n",
        "testData.cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[21] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzkZFMrLFYGV"
      },
      "source": [
        "def modelEval(model, dataset):\n",
        "    \n",
        "    # All artists in the 'userArtistData' dataset\n",
        "\n",
        "    AllArtists = spark.parallelize(set(userArtistData.map(lambda x:x[1]).collect()))\n",
        "    \n",
        "    \n",
        "    # Set of all users in the current (Validation/Testing) dataset\n",
        "\n",
        "    AllUsers = spark.parallelize(set(dataset.map(lambda x:x[0]).collect()))\n",
        "    \n",
        "    \n",
        "    # Create a dictionary of (key, values) for current (Validation/Testing) dataset\n",
        "\n",
        "    ValidationAndTestingDictionary ={}\n",
        "    for temp in AllUsers.collect():\n",
        "        tempFilter = dataset.filter(lambda x:x[0] == temp).collect()\n",
        "        for item in tempFilter:\n",
        "            if temp in ValidationAndTestingDictionary:\n",
        "                ValidationAndTestingDictionary[temp].append(item[1])\n",
        "            else:\n",
        "                ValidationAndTestingDictionary[temp] = [item[1]]\n",
        "                    \n",
        "    \n",
        "    # Create a dictionary of (key, values) for training dataset\n",
        "\n",
        "    TrainingDictionary = {}\n",
        "    for temp in AllUsers.collect():\n",
        "        tempFilter = trainData.filter(lambda x:x[0] == temp).collect()\n",
        "        for item in tempFilter:\n",
        "            if temp in TrainingDictionary:\n",
        "                TrainingDictionary[temp].append(item[1])\n",
        "            else:\n",
        "                TrainingDictionary[temp] = [item[1]]\n",
        "        \n",
        "    \n",
        "    # For each user, calculate the prediction score i.e. similarity between predicted and actual artists\n",
        "\n",
        "    PredictionScore = 0.00\n",
        "    for temp in AllUsers.collect():\n",
        "        ArtistPrediction =  AllArtists.map(lambda x:(temp,x))\n",
        "        ModelPrediction = model.predictAll(ArtistPrediction)\n",
        "        tempFilter = ModelPrediction.filter(lambda x :not x[1] in TrainingDictionary[x[0]])\n",
        "        topPredictions = tempFilter.top(len(ValidationAndTestingDictionary[temp]),key=lambda x:x[2])\n",
        "        l=[]\n",
        "        for i in topPredictions:\n",
        "            l.append(i[1])\n",
        "        PredictionScore+=len(set(l).intersection(ValidationAndTestingDictionary[temp]))/len(ValidationAndTestingDictionary[temp])    \n",
        "\n",
        "        \n",
        "    # Print average score of the model for all users for the specified rank\n",
        "\n",
        "    print(\"The model score for rank \"+str(model.rank)+\" is ~\"+str(PredictionScore/len(ValidationAndTestingDictionary)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGW4_5MlFrLA",
        "outputId": "4f20857d-0e26-489a-93e7-a088c84bf1b0"
      },
      "source": [
        "rankList = [2,10,20]\n",
        "for rank in rankList:\n",
        "    model = ALS.trainImplicit(trainData, rank , seed=345)\n",
        "    modelEval(model,validationData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model score for rank 2 is ~0.08082178719723072\n",
            "The model score for rank 10 is ~0.09052071953413846\n",
            "The model score for rank 20 is ~0.08225274139572855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDklhCw0Gg_n",
        "outputId": "272f9038-aa83-4f1b-b705-7ab225a34bc5"
      },
      "source": [
        "bestModel = ALS.trainImplicit(trainData, rank=10, seed=345)\n",
        "modelEval(bestModel, testData)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model score for rank 10 is ~0.060728260020964896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qHDCTM1Gxsv",
        "outputId": "80de8f3e-547e-4ff3-9ffa-9fb89d9a1a7b"
      },
      "source": [
        "TopFive = bestModel.recommendProducts(2064012,5)\n",
        "for item in range(0,5):\n",
        "    print(\"Artist \"+str(item)+\": \"+artistData.filter(lambda x:x[0] == TopFive[item][1]).collect()[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Artist 0: Rage Against the Machine\n",
            "Artist 1: Franz Ferdinand\n",
            "Artist 2: U2\n",
            "Artist 3: Pink Floyd\n",
            "Artist 4: Led Zeppelin\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}